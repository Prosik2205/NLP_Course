{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741c330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['low', 'low', 'low', 'lower', 'lower', 'lowest', 'the', 'the', 'the', 'quick', 'quick', 'brown', 'fox']\n",
      "Word frequencies: Counter({'low': 3, 'the': 3, 'lower': 2, 'quick': 2, 'lowest': 1, 'brown': 1, 'fox': 1})\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Implementing BPE from Scratch\n",
    "# Step 1: Understanding the Data Structures\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "def preprocess_text(text):\n",
    " \"\"\"\n",
    " Lowercase and split text into words.\n",
    " \"\"\"\n",
    " # Lowercase\n",
    " text = text.lower()\n",
    " # Split on whitespace and punctuation\n",
    " words = re.findall(r'\\w+', text)\n",
    " return words\n",
    "\n",
    "corpus = \"\"\"\n",
    "low low low lower lower lowest\n",
    "the the the quick quick brown fox\n",
    "\"\"\"\n",
    "words = preprocess_text(corpus)\n",
    "print(\"Words:\", words)\n",
    "\n",
    "#Count word frequencies\n",
    "word_freqs = Counter(words)\n",
    "print(\"Word frequencies:\", word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb0d2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary:\n",
      " low: ['l', 'o', 'w', '</w>']\n",
      " lower: ['l', 'o', 'w', 'e', 'r', '</w>']\n",
      " lowest: ['l', 'o', 'w', 'e', 's', 't', '</w>']\n",
      " the: ['t', 'h', 'e', '</w>']\n",
      " quick: ['q', 'u', 'i', 'c', 'k', '</w>']\n",
      " brown: ['b', 'r', 'o', 'w', 'n', '</w>']\n",
      " fox: ['f', 'o', 'x', '</w>']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize Character-Level Vocabulary\n",
    "def get_vocab(word_freqs):\n",
    " \"\"\"\n",
    " Split each word into characters + </w> marker.\n",
    " Returns dictionary: word -> list of tokens\n",
    " \"\"\"\n",
    " vocab = {}\n",
    " for word, freq in word_freqs.items():\n",
    "    # Split into characters, add </w> marker\n",
    "    vocab[word] = list(word) + ['</w>']\n",
    " return vocab\n",
    "vocab = get_vocab(word_freqs)\n",
    "print(\"Initial vocabulary:\")\n",
    "for word, tokens in vocab.items():\n",
    " print(f\" {word}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "963b4c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair frequencies:\n",
      " ('o', 'w'): 7\n",
      " ('l', 'o'): 6\n",
      " ('w', '</w>'): 3\n",
      " ('w', 'e'): 3\n",
      " ('t', 'h'): 3\n",
      " ('h', 'e'): 3\n",
      " ('e', '</w>'): 3\n",
      " ('e', 'r'): 2\n",
      " ('r', '</w>'): 2\n",
      " ('q', 'u'): 2\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Count Pairs\n",
    "def get_stats(vocab, word_freqs):\n",
    " \"\"\"\n",
    " Count frequency of adjacent token pairs.\n",
    " \"\"\"\n",
    " pairs = defaultdict(int)\n",
    " for word, freq in word_freqs.items():\n",
    "    symbols = vocab[word]\n",
    "    for i in range(len(symbols) - 1):\n",
    "        pair = (symbols[i], symbols[i+1])\n",
    "        pairs[pair] += freq\n",
    " return pairs\n",
    "\n",
    "pairs = get_stats(vocab, word_freqs)\n",
    "print(\"Pair frequencies:\")\n",
    "for pair, freq in sorted(pairs.items(), key=lambda x: -x[1])[:10]:\n",
    " print(f\" {pair}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c02366ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging: ('o', 'w') → ow\n",
      "Vocabulary after merge:\n",
      " low: ['l', 'ow', '</w>']\n",
      " lower: ['l', 'ow', 'e', 'r', '</w>']\n",
      " lowest: ['l', 'ow', 'e', 's', 't', '</w>']\n",
      " the: ['t', 'h', 'e', '</w>']\n",
      " quick: ['q', 'u', 'i', 'c', 'k', '</w>']\n",
      " brown: ['b', 'r', 'ow', 'n', '</w>']\n",
      " fox: ['f', 'o', 'x', '</w>']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Merge Most Frequent Pair\n",
    "def merge_vocab(pair, vocab):\n",
    " \"\"\"\n",
    " Merge the given pair in the vocabulary.\n",
    " \"\"\"\n",
    " new_vocab = {}\n",
    "\n",
    " # Create pattern to find the pair\n",
    " # Example: ('l', 'o') -> 'l o'\n",
    " pattern = ' '.join(pair)\n",
    " replacement = ''.join(pair)\n",
    " for word, tokens in vocab.items():\n",
    "    # Convert tokens list to string for replacement\n",
    "    tokens_str = ' '.join(tokens)\n",
    "    # Replace first occurrence of pair\n",
    "    tokens_str = tokens_str.replace(pattern, replacement)\n",
    "    # Convert back to list\n",
    "    new_vocab[word] = tokens_str.split()\n",
    " return new_vocab\n",
    "\n",
    "most_frequent_pair = max(pairs, key=pairs.get)\n",
    "print(f\"\\nMerging: {most_frequent_pair} → {''.join(most_frequent_pair)}\")\n",
    "\n",
    "vocab = merge_vocab(most_frequent_pair, vocab)\n",
    "print(\"Vocabulary after merge:\")\n",
    "for word, tokens in vocab.items():\n",
    " print(f\" {word}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "644fb800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Merged ('o', 'w') → ow(freq=7)\n",
      "Iteration 2: Merged ('l', 'ow') → low(freq=6)\n",
      "Iteration 3: Merged ('low', '</w>') → low</w>(freq=3)\n",
      "Iteration 4: Merged ('low', 'e') → lowe(freq=3)\n",
      "Iteration 5: Merged ('t', 'h') → th(freq=3)\n",
      "Iteration 6: Merged ('th', 'e') → the(freq=3)\n",
      "Iteration 7: Merged ('the', '</w>') → the</w>(freq=3)\n",
      "Iteration 8: Merged ('lowe', 'r') → lower(freq=2)\n",
      "Iteration 9: Merged ('lower', '</w>') → lower</w>(freq=2)\n",
      "Iteration 10: Merged ('q', 'u') → qu(freq=2)\n",
      "\n",
      "Final vocabulary:\n",
      " low: ['low</w>']\n",
      " lower: ['lower</w>']\n",
      " lowest: ['lowe', 's', 't', '</w>']\n",
      " the: ['the</w>']\n",
      " quick: ['qu', 'i', 'c', 'k', '</w>']\n",
      " brown: ['b', 'r', 'ow', 'n', '</w>']\n",
      " fox: ['f', 'o', 'x', '</w>']\n",
      "\n",
      "Merge operations (in order):\n",
      " 1. o + w → ow\n",
      " 2. l + ow → low\n",
      " 3. low + </w> → low</w>\n",
      " 4. low + e → lowe\n",
      " 5. t + h → th\n",
      " 6. th + e → the\n",
      " 7. the + </w> → the</w>\n",
      " 8. lowe + r → lower\n",
      " 9. lower + </w> → lower</w>\n",
      " 10. q + u → qu\n"
     ]
    }
   ],
   "source": [
    "#Step 6: Complete BPE Training Loop\n",
    "def train_bpe(word_freqs, num_merges):\n",
    " \"\"\"\n",
    " Train BPE tokenizer for num_merges iterations.\n",
    " Returns vocabulary and list of merge operations.\n",
    " \"\"\"\n",
    " vocab = get_vocab(word_freqs)\n",
    " merges = []\n",
    " for i in range(num_merges):\n",
    "    pairs = get_stats(vocab, word_freqs)\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # Find most frequent pair\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "\n",
    "    # Merge it\n",
    "    vocab = merge_vocab(best_pair, vocab)\n",
    "    merges.append(best_pair)\n",
    "    print(f\"Iteration {i+1}: Merged {best_pair} → {''.join(best_pair)}(freq={pairs[best_pair]})\")\n",
    " return vocab, merges\n",
    "\n",
    "vocab, merges = train_bpe(word_freqs, num_merges=10)\n",
    "\n",
    "print(\"\\nFinal vocabulary:\")\n",
    "for word, tokens in vocab.items():\n",
    " print(f\" {word}: {tokens}\")\n",
    " \n",
    "print(\"\\nMerge operations (in order):\")\n",
    "for i, merge in enumerate(merges, 1):\n",
    " print(f\" {i}. {merge[0]} + {merge[1]} → {''.join(merge)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b02de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding test words:\n",
      " lower → ['l', 'ow', 'e', 'r', '</w>']\n",
      " lowest → ['l', 'ow', 'e', 's', 't', '</w>']\n",
      " newer → ['n', 'e', 'w', 'e', 'r', '</w>']\n"
     ]
    }
   ],
   "source": [
    "# Step 7: BPE Encoding (Inference)\n",
    "def encode_word(word, merges):\n",
    " \"\"\"\n",
    " Encode a word using learned BPE merges.\n",
    " \"\"\"\n",
    " # Start with character-level tokens\n",
    " tokens = list(word) + ['</w>']\n",
    " # Apply merges in order\n",
    " for merge in merges:\n",
    "    pair = merge\n",
    "    if pair not in zip(tokens[:-1], tokens[1:]):\n",
    "       continue\n",
    " \n",
    "    # Find and merge all occurrences\n",
    "    i = 0\n",
    "    while i < len(tokens) - 1:\n",
    "        if (tokens[i], tokens[i+1]) == pair:\n",
    "            # Merge\n",
    "            tokens = tokens[:i] + [''.join(pair)] + tokens[i+2:]\n",
    "        else:\n",
    "            i += 1\n",
    "    return toke\n",
    "\n",
    "def encode_word_better(word, merges):\n",
    " \"\"\"\n",
    " Encode a word by applying merges in order.\n",
    " \n",
    " \"\"\"\n",
    " tokens = list(word) + ['</w>']\n",
    " \n",
    " for pair in merges:\n",
    "    i = 0\n",
    "    new_tokens = []\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "            new_tokens.append(''.join(pair))\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    tokens = new_tokens    \n",
    " return tokens\n",
    "\n",
    "test_words = ['lower', 'lowest', 'newer']\n",
    "\n",
    "print(\"\\nEncoding test words:\")\n",
    "for word in test_words:\n",
    " encoded = encode_word_better(word, merges)\n",
    " print(f\" {word} → {encoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cefffadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained BPE with 15 merges\n",
      "Vocabulary size: 33\n",
      "\n",
      "Encoded: the quick brown fox jumps lower\n",
      "Tokens: ['the</w>', 'quick</w>', 'b', 'r', 'ow', 'n', '</w>', 'f', 'o', 'x', '</w>', 'j', 'u', 'm', 'p', 's', '</w>', 'lower</w>']\n",
      "Token count: 18\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Complete BPE Tokenizer Class\n",
    "class SimpleBPETokenizer:\n",
    " def __init__(self):\n",
    "    self.merges = []\n",
    "    self.vocab = set()\n",
    "\n",
    " def train(self, text, num_merges):\n",
    "    \"\"\"Train BPE on text.\"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    word_freqs = Counter(words)\n",
    "\n",
    "    vocab = get_vocab(word_freqs)\n",
    "    \n",
    "    # Extract initial vocabulary\n",
    "    for tokens in vocab.values():\n",
    "        self.vocab.update(tokens)\n",
    "    \n",
    "    # Perform merges\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab, word_freqs)\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        self.merges.append(best_pair)\n",
    "        self.vocab.add(''.join(best_pair))\n",
    "\n",
    "    print(f\"Trained BPE with {len(self.merges)} merges\")\n",
    "    print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    " def encode(self, text):\n",
    "    \"\"\"Encode text into BPE tokens.\"\"\"\n",
    "    words = preprocess_text(text)\n",
    "    all_tokens = []\n",
    "    for word in words:\n",
    "        tokens = encode_word_better(word, self.merges)\n",
    "        all_tokens.extend(tokens)\n",
    "    return all_tokens\n",
    "\n",
    " def get_vocab_size(self):\n",
    "    return len(self.vocab)\n",
    "\n",
    "tokenizer = SimpleBPETokenizer()\n",
    "tokenizer.train(corpus, num_merges=15)\n",
    "\n",
    "test_text = \"the quick brown fox jumps lower\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "print(f\"\\nEncoded: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5df36c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prosi\\Documents\\GitHub\\NLP_Course\\envire\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\Users\\prosi\\Documents\\GitHub\\NLP_Course\\envire\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\prosi\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT WordPiece tokens: ['playing', 'with', 'bert', 'token', '##ization', 'is', 'fun', '!']\n",
      "Token IDs: [101, 2652, 2007, 14324, 19204, 3989, 2003, 4569, 999, 102]\n",
      "Decoded: [CLS] playing with bert tokenization is fun! [SEP]\n",
      "BERT vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Using Existing Libraries\n",
    "# Using Hugging Face Transformers for WordPiece (BERT)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"Playing with BERT tokenization is fun!\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"BERT WordPiece tokens:\", tokens)\n",
    "\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(\"Decoded:\", decoded)\n",
    "\n",
    "print(f\"BERT vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0434327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece model trained!\n",
      "\n",
      "SentencePiece tokens: ['▁Natural', '▁language', '▁processing']\n",
      "Token IDs: [146, 153, 157]\n",
      "Decoded: Natural language processing\n",
      "Decoded from IDs: Natural language processing\n",
      "SentencePiece vocab size: 300\n",
      "\n",
      "First 20 tokens in vocabulary:\n",
      " 0: <pad>\n",
      " 1: <unk>\n",
      " 2: <s>\n",
      " 3: </s>\n",
      " 4: in\n",
      " 5: en\n",
      " 6: ▁t\n",
      " 7: at\n",
      " 8: ce\n",
      " 9: he\n",
      " 10: ar\n",
      " 11: iz\n",
      " 12: ok\n",
      " 13: ro\n",
      " 14: ▁f\n",
      " 15: ▁l\n",
      " 16: ▁p\n",
      " 17: ing\n",
      " 18: eniz\n",
      " 19: ▁the\n",
      "Text: tokenization strategies\n",
      "BPE: ['▁tokenization', '▁st', 'r', 'at', 'e', 'g', 'i', 'es']\n",
      "Unigram: ['▁tokeniz', 'ati', 'o', 'n', '▁', 'st', 'r', 'at', 'e', 'g', 'i', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "#Using SentencePiece\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "training_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Natural language processing is fascinating.\n",
    "Tokenization is the first step in NLP.\n",
    "Subword tokenization solves the OOV problem.\n",
    "BPE, WordPiece, and SentencePiece are popular methods.\n",
    "Machine learning models need tokenized input.\n",
    "\"\"\" * 100 # Repeat for more data\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    " f.write(training_text)\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    " input='train.txt',\n",
    " model_prefix='sp_model',\n",
    " vocab_size=300,\n",
    " model_type='bpe', # or 'unigram'\n",
    " character_coverage=1.0,\n",
    " pad_id=0,\n",
    " unk_id=1,\n",
    " bos_id=2,\n",
    " eos_id=3\n",
    ")\n",
    "\n",
    "print(\"SentencePiece model trained!\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('sp_model.model')\n",
    "\n",
    "test_text = \"Natural language processing\"\n",
    "tokens = sp.encode_as_pieces(test_text)\n",
    "print(f\"\\nSentencePiece tokens: {tokens}\")\n",
    "\n",
    "ids = sp.encode_as_ids(test_text)\n",
    "print(f\"Token IDs: {ids}\")\n",
    "\n",
    "decoded = sp.decode_pieces(tokens)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "decoded_ids = sp.decode_ids(ids)\n",
    "print(f\"Decoded from IDs: {decoded_ids}\")\n",
    "\n",
    "print(f\"SentencePiece vocab size: {sp.vocab_size()}\")\n",
    "\n",
    "print(\"\\nFirst 20 tokens in vocabulary:\")\n",
    "for i in range(20):\n",
    " print(f\" {i}: {sp.id_to_piece(i)}\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    " input='train.txt',\n",
    " model_prefix='sp_unigram',\n",
    " vocab_size=80,\n",
    " model_type='unigram', # Unigram LM\n",
    " character_coverage=1.0\n",
    ")\n",
    "sp_unigram = spm.SentencePieceProcessor()\n",
    "sp_unigram.load('sp_unigram.model')\n",
    "\n",
    "test_text = \"tokenization strategies\"\n",
    "bpe_tokens = sp.encode_as_pieces(test_text)\n",
    "\n",
    "unigram_tokens = sp_unigram.encode_as_pieces(test_text)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"BPE: {bpe_tokens}\")\n",
    "print(f\"Unigram: {unigram_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "704cba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOKENIZATION COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "--------------------------------------------------------------------------------\n",
      "Simple BPE: [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102]...\n",
      " Token count: 12\n",
      "BERT WordPiece: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      " Token count: 10\n",
      "SentencePiece: ['▁The', '▁quick', '▁brown', '▁fox', '▁jumps', '▁over', '▁the', '▁lazy', '▁dog', '.']\n",
      " Token count: 10\n",
      "\n",
      "Original: Natural language processing is fascinating!\n",
      "--------------------------------------------------------------------------------\n",
      "Simple BPE: [101, 3019, 2653, 6364, 2003, 17160, 999, 102]...\n",
      " Token count: 8\n",
      "BERT WordPiece: ['natural', 'language', 'processing', 'is', 'fascinating', '!']\n",
      " Token count: 6\n",
      "SentencePiece: ['▁Natural', '▁language', '▁processing', '▁is', '▁fascinating', '!']\n",
      " Token count: 6\n",
      "\n",
      "Original: Subword tokenization: BPE, WordPiece, SentencePiece.\n",
      "--------------------------------------------------------------------------------\n",
      "Simple BPE: [101, 4942, 18351, 19204, 3989, 1024, 17531, 2063, 1010, 2773, 11198, 1010, 6251, 11198, 1012]...\n",
      " Token count: 16\n",
      "BERT WordPiece: ['sub', '##word', 'token', '##ization', ':', 'bp', '##e', ',', 'word', '##piece', ',', 'sentence', '##piece', '.']\n",
      " Token count: 14\n",
      "SentencePiece: ['▁Subword', '▁tokenization', ':', '▁BPE', ',', '▁WordPiece', ',', '▁SentencePiece', '.']\n",
      " Token count: 9\n",
      "\n",
      "Original: COVID-19 pandemic affected the world in 2020.\n",
      "--------------------------------------------------------------------------------\n",
      "Simple BPE: [101, 2522, 17258, 1011, 2539, 6090, 3207, 7712, 5360, 1996, 2088, 1999, 12609, 1012, 102]...\n",
      " Token count: 15\n",
      "BERT WordPiece: ['co', '##vid', '-', '19', 'pan', '##de', '##mic', 'affected', 'the', 'world', 'in', '2020', '.']\n",
      " Token count: 13\n",
      "SentencePiece: ['▁', 'C', 'OV', 'ID-19', '▁p', 'and', 'em', 'ic', '▁a', 'f', 'f', 'ec', 't', 'ed', '▁the', '▁', 'w', 'or', 'l', 'd', '▁in', '▁', '2020', '.']\n",
      " Token count: 24\n",
      "\n",
      "Original: Machine learning models require tokenized input.\n",
      "--------------------------------------------------------------------------------\n",
      "Simple BPE: [101, 3698, 4083, 4275, 5478, 19204, 3550, 7953, 1012, 102]...\n",
      " Token count: 10\n",
      "BERT WordPiece: ['machine', 'learning', 'models', 'require', 'token', '##ized', 'input', '.']\n",
      " Token count: 8\n",
      "SentencePiece: ['▁Machine', '▁learning', '▁models', '▁', 're', 'qu', 'ir', 'e', '▁tokenized', '▁input', '.']\n",
      " Token count: 11\n",
      "\n",
      "Original: Typo example: recieve instead of receive.\n",
      "--------------------------------------------------------------------------------\n",
      "Simple BPE: [101, 5939, 6873, 2742, 1024, 28667, 2666, 3726, 2612, 1997, 4374, 1012, 102]...\n",
      " Token count: 13\n",
      "BERT WordPiece: ['ty', '##po', 'example', ':', 'rec', '##ie', '##ve', 'instead', 'of', 'receive', '.']\n",
      " Token count: 11\n",
      "SentencePiece: ['▁T', 'y', 'po', '▁', 'e', 'x', 'a', 'mp', 'le', ':', '▁', 're', 'ci', 'e', 've', '▁in', 'st', 'ea', 'd', '▁o', 'f', '▁', 're', 'ce', 'i', 've', '.']\n",
      " Token count: 27\n",
      "\n",
      "Original: Scientific term: deoxyribonucleic acid (DNA).\n",
      "--------------------------------------------------------------------------------\n",
      "Simple BPE: [101, 4045, 2744, 1024, 2139, 11636, 12541, 12322, 2239, 14194, 23057, 2278, 5648, 1006, 6064]...\n",
      " Token count: 18\n",
      "BERT WordPiece: ['scientific', 'term', ':', 'de', '##ox', '##yr', '##ib', '##on', '##uc', '##lei', '##c', 'acid', '(', 'dna', ')', '.']\n",
      " Token count: 16\n",
      "SentencePiece: ['▁S', 'ci', 'ent', 'i', 'fi', 'c', '▁t', 'er', 'm', ':', '▁', 'de', 'ox', 'y', 'r', 'i', 'b', 'on', 'u', 'c', 'le', 'ic', '▁', 'ac', 'i', 'd', '▁', '(D', 'N', 'A)', '.']\n",
      " Token count: 31\n"
     ]
    }
   ],
   "source": [
    "#Part 3: Comparison and Analysis\n",
    "\n",
    "test_sentences = [\n",
    " \"The quick brown fox jumps over the lazy dog.\",\n",
    " \"Natural language processing is fascinating!\",\n",
    " \"Subword tokenization: BPE, WordPiece, SentencePiece.\",\n",
    " \"COVID-19 pandemic affected the world in 2020.\",\n",
    " \"Machine learning models require tokenized input.\",\n",
    " \"Typo example: recieve instead of receive.\",\n",
    " \"Scientific term: deoxyribonucleic acid (DNA).\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOKENIZATION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for sent in test_sentences:\n",
    " print(f\"\\nOriginal: {sent}\")\n",
    " print(\"-\" * 80)\n",
    "\n",
    " # Our simple BPE\n",
    " simple_tokens = tokenizer.encode(sent)\n",
    " print(f\"Simple BPE: {simple_tokens[:15]}...\") # First 15 tokens\n",
    " print(f\" Token count: {len(simple_tokens)}\")\n",
    " \n",
    " # BERT WordPiece\n",
    " bert_tokens = BertTokenizer.from_pretrained('bert-base-uncased').tokenize(sent)\n",
    " print(f\"BERT WordPiece: {bert_tokens}\")\n",
    " print(f\" Token count: {len(bert_tokens)}\")\n",
    " \n",
    " # SentencePiece BPE\n",
    " sp_tokens = sp.encode_as_pieces(sent)\n",
    " print(f\"SentencePiece: {sp_tokens}\")\n",
    " print(f\" Token count: {len(sp_tokens)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cecc824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FERTILITY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog....\n",
      " Words: 9\n",
      " BERT WordPiece: 10 tokens → fertility = 1.11\n",
      " SentencePiece: 10 tokens → fertility = 1.11\n",
      "\n",
      "Sentence: Natural language processing is fascinating!...\n",
      " Words: 5\n",
      " BERT WordPiece: 6 tokens → fertility = 1.20\n",
      " SentencePiece: 6 tokens → fertility = 1.20\n",
      "\n",
      "Sentence: Subword tokenization: BPE, WordPiece, SentencePiec...\n",
      " Words: 5\n",
      " BERT WordPiece: 14 tokens → fertility = 2.80\n",
      " SentencePiece: 9 tokens → fertility = 1.80\n",
      "\n",
      "Sentence: COVID-19 pandemic affected the world in 2020....\n",
      " Words: 7\n",
      " BERT WordPiece: 13 tokens → fertility = 1.86\n",
      " SentencePiece: 24 tokens → fertility = 3.43\n",
      "\n",
      "Sentence: Machine learning models require tokenized input....\n",
      " Words: 6\n",
      " BERT WordPiece: 8 tokens → fertility = 1.33\n",
      " SentencePiece: 11 tokens → fertility = 1.83\n",
      "\n",
      "Sentence: Typo example: recieve instead of receive....\n",
      " Words: 6\n",
      " BERT WordPiece: 11 tokens → fertility = 1.83\n",
      " SentencePiece: 27 tokens → fertility = 4.50\n",
      "\n",
      "Sentence: Scientific term: deoxyribonucleic acid (DNA)....\n",
      " Words: 5\n",
      " BERT WordPiece: 16 tokens → fertility = 3.20\n",
      " SentencePiece: 31 tokens → fertility = 6.20\n"
     ]
    }
   ],
   "source": [
    "def calculate_fertility(text, tokenizer_func):\n",
    " \"\"\"\n",
    " Calculate fertility: avg tokens per word.\n",
    " \"\"\"\n",
    " words = text.split()\n",
    " tokens = tokenizer_func(text)\n",
    "\n",
    " # Remove special tokens\n",
    " tokens = [t for t in tokens if t not in ['<s>', '</s>', '[CLS]', '[SEP]', '<pad>']]\n",
    " fertility = len(tokens) / len(words) if words else 0\n",
    " return fertility, len(words), len(tokens)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FERTILITY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for sent in test_sentences:\n",
    " print(f\"\\nSentence: {sent[:50]}...\")\n",
    "\n",
    " # BERT\n",
    " bert_fert, words, bert_tokens = calculate_fertility(sent, lambda t: BertTokenizer.from_pretrained('bert-base-uncased').tokenize(t))\n",
    "\n",
    " # SentencePiece\n",
    " sp_fert, _, sp_tokens = calculate_fertility(sent,lambda t: sp.encode_as_pieces(t))\n",
    "\n",
    " print(f\" Words: {words}\")\n",
    " print(f\" BERT WordPiece: {bert_tokens} tokens → fertility = {bert_fert:.2f}\")\n",
    " print(f\" SentencePiece: {sp_tokens} tokens → fertility = {sp_fert:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59ed82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OUT-OF-DOMAIN VOCABULARY COVERAGE\n",
      "================================================================================\n",
      "Text: \n",
      "Azithromycin is a macrolide antibiotic used to treat pneumonia.\n",
      "Deoxyribonucleic acid stores geneti...\n",
      "\n",
      "BERT WordPiece:\n",
      " Coverage: 100.0%\n",
      " Unknown tokens: 0/31\n"
     ]
    }
   ],
   "source": [
    "def analyze_vocabulary_coverage(text, tokenizer, unk_token):\n",
    " \"\"\"\n",
    " Calculate percentage of unknown tokens.\n",
    " \"\"\"\n",
    " tokens = tokenizer.tokenize(text)\n",
    " unk_count = tokens.count(unk_token)\n",
    " total = len(tokens)\n",
    " coverage = (1 - unk_count / total) * 100 if total > 0 else 0\n",
    " return coverage, unk_count, total\n",
    "\n",
    "medical_text = \"\"\"\n",
    "Azithromycin is a macrolide antibiotic used to treat pneumonia.\n",
    "Deoxyribonucleic acid stores genetic information in chromosomes.\n",
    "\"\"\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUT-OF-DOMAIN VOCABULARY COVERAGE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Text: {medical_text[:100]}...\")\n",
    "bert_cov, bert_unk, bert_total = analyze_vocabulary_coverage(medical_text, BertTokenizer.from_pretrained('bert-base-uncased'),'[UNK]')\n",
    "print(f\"\\nBERT WordPiece:\")\n",
    "print(f\" Coverage: {bert_cov:.1f}%\")\n",
    "print(f\" Unknown tokens: {bert_unk}/{bert_total}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b263c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOKENIZATION SPEED COMPARISON\n",
      "================================================================================\n",
      "Text length: 3240 characters\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT WordPiece: 819.31 ms/run\n",
      "SentencePiece: 0.83 ms/run\n",
      "Speedup: 987.72x\n"
     ]
    }
   ],
   "source": [
    "#Timing Comparison\n",
    "import time\n",
    "def benchmark_tokenizer(text, tokenize_func, num_runs=100):\n",
    " \"\"\"\n",
    " Benchmark tokenization speed.\n",
    " \"\"\"\n",
    " start = time.time()\n",
    " for _ in range(num_runs):\n",
    "    tokens = tokenize_func(text)\n",
    " end = time.time()\n",
    "\n",
    " avg_time = (end - start) / num_runs * 1000 # milliseconds\n",
    " return avg_time\n",
    "\n",
    "long_text = \" \".join(test_sentences) * 10\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOKENIZATION SPEED COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Text length: {len(long_text)} characters\\n\")\n",
    "\n",
    "bert_time = benchmark_tokenizer(\n",
    " long_text,\n",
    " lambda t: BertTokenizer.from_pretrained('bert-base-uncased').tokenize(t)\n",
    ")\n",
    "sp_time = benchmark_tokenizer(\n",
    " long_text,\n",
    " lambda t: sp.encode_as_pieces(t)\n",
    ")\n",
    "print(f\"BERT WordPiece: {bert_time:.2f} ms/run\")\n",
    "print(f\"SentencePiece: {sp_time:.2f} ms/run\")\n",
    "print(f\"Speedup: {bert_time/sp_time:.2f}x\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
