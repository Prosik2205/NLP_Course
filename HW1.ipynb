{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78b71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK version: 3.9.2\n",
      "spaCy version: 3.8.11\n",
      "scikit-learn version: 1.8.0\n",
      "spaCy model loaded: core_web_sm\n"
     ]
    }
   ],
   "source": [
    "#SETUP & INSTALLATION\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"NLTK version: {nltk.__version__}\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Test spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello, world!\")\n",
    "print(f\"spaCy model loaded: {nlp.meta['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc7589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 7\n",
      "\n",
      "Sentence 1: Natural Language Processing is fascinating.\n",
      "Sentence 2: It enables computers\n",
      "to understand human language.\n",
      "Sentence 3: Dr. Smith works at N.A.S.A.\n",
      "Sentence 4: on text analysis.\n",
      "Sentence 5: He said, \"NLP is the\n",
      "future!\"\n",
      "Sentence 6: What do you think?\n",
      "Sentence 7: Visit www.nlp.org for more info.\n"
     ]
    }
   ],
   "source": [
    "#Ex1\n",
    "#1.1\n",
    "\n",
    "import nltk\n",
    "# Download required NLTK data (run once)\n",
    "nltk.data.path.append(r\"C:\\Users\\prosi\\AppData\\Roaming\\nltk_data\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"Natural Language Processing is fascinating. It enables computers\n",
    "to understand human language.\n",
    "Dr. Smith works at N.A.S.A. on text analysis. He said, \"NLP is the\n",
    "future!\"\n",
    "What do you think? Visit www.nlp.org for more info.\"\"\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Number of sentences: {len(sentences)}\\n\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    " print(f\"Sentence {i}: {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96aa5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 15\n",
      "\n",
      "Tokens: ['Do', \"n't\", 'forget', ':', 'pre-processing', 'costs', '$', '100-', '$', '200', '!', 'Email', 'john', '@', 'example.com']\n"
     ]
    }
   ],
   "source": [
    "#1.2\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"Don't forget: pre-processing costs $100-$200! Email john@example.com\"\n",
    "# Tokenize into words\n",
    "tokens = word_tokenize(sentence)\n",
    "print(f\"Number of tokens: {len(tokens)}\\n\")\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "848cf7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard word_tokenize:\n",
      "['We', \"'re\", 'analyzing', 'BERT', \"'s\", 'performance', 'on', 'GPT-3.5', '.', 'Wow', '!']\n",
      "\n",
      "WordPunct tokenizer:\n",
      "['We', \"'\", 're', 'analyzing', 'BERT', \"'\", 's', 'performance', 'on', 'GPT', '-', '3', '.', '5', '.', 'Wow', '!']\n",
      "\n",
      "Treebank tokenizer:\n",
      "['We', \"'re\", 'analyzing', 'BERT', \"'s\", 'performance', 'on', 'GPT-3.5.', 'Wow', '!']\n"
     ]
    }
   ],
   "source": [
    "#1.3\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize,TreebankWordTokenizer\n",
    "text = \"We're analyzing BERT's performance on GPT-3.5. Wow!\"\n",
    "# Different tokenizers\n",
    "standard = word_tokenize(text)\n",
    "wordpunct = wordpunct_tokenize(text)\n",
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "print(\"Standard word_tokenize:\")\n",
    "print(standard)\n",
    "print(f\"\\nWordPunct tokenizer:\")\n",
    "print(wordpunct)\n",
    "print(f\"\\nTreebank tokenizer:\")\n",
    "print(treebank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab5e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with POS tags and lemmas:\n",
      "\n",
      "Apple           | POS: PROPN    | Lemma:Apple           | Is_alpha: True\n",
      "Inc.            | POS: PROPN    | Lemma:Inc.            | Is_alpha: False\n",
      "is              | POS: AUX      | Lemma:be              | Is_alpha: True\n",
      "looking         | POS: VERB     | Lemma:look            | Is_alpha: True\n",
      "at              | POS: ADP      | Lemma:at              | Is_alpha: True\n",
      "buying          | POS: VERB     | Lemma:buy             | Is_alpha: True\n",
      "U.K.            | POS: PROPN    | Lemma:U.K.            | Is_alpha: False\n",
      "startup         | POS: VERB     | Lemma:startup         | Is_alpha: True\n",
      "for             | POS: ADP      | Lemma:for             | Is_alpha: True\n",
      "$               | POS: SYM      | Lemma:$               | Is_alpha: False\n",
      "1               | POS: NUM      | Lemma:1               | Is_alpha: False\n",
      "billion         | POS: NUM      | Lemma:billion         | Is_alpha: True\n",
      ".               | POS: PUNCT    | Lemma:.               | Is_alpha: False\n",
      "CEO             | POS: NOUN     | Lemma:ceo             | Is_alpha: True\n",
      "Tim             | POS: PROPN    | Lemma:Tim             | Is_alpha: True\n",
      "Cook            | POS: PROPN    | Lemma:Cook            | Is_alpha: True\n",
      "confirmed       | POS: VERB     | Lemma:confirm         | Is_alpha: True\n",
      "it              | POS: PRON     | Lemma:it              | Is_alpha: True\n",
      ".               | POS: PUNCT    | Lemma:.               | Is_alpha: False\n"
     ]
    }
   ],
   "source": [
    "#1.4\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple Inc. is looking at buying U.K. startup for $1 billion. CEO Tim Cook confirmed it.\"\n",
    "doc = nlp(text)\n",
    "print(\"Tokens with POS tags and lemmas:\\n\")\n",
    "for token in doc:\n",
    " print(f\"{token.text:15} | POS: {token.pos_:8} | Lemma:{token.lemma_:15} | Is_alpha: {token.is_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 4\n",
      "Total tokens: 33\n",
      "Unique tokens (types): 29\n",
      "\n",
      "URL handling:\n",
      "['https']\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 1: Your Turn ?????????\n",
    "text = \"\"\"\n",
    "The COVID-19 pandemic started in 2019-2020. Dr. Johnson said, \"We're\n",
    "making progress!\"\n",
    "The vaccine costs €50-€100 in the E.U. Visit https://who.int for updates.\n",
    "\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# 1. Sentence count\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentences: {len(sentences)}\")\n",
    "# 2. Word tokens\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "# 3. Unique tokens\n",
    "unique_tokens = set(tokens)\n",
    "print(f\"Unique tokens (types): {len(unique_tokens)}\")\n",
    "# 4. Check URL\n",
    "print(f\"\\nURL handling:\")\n",
    "print([t for t in tokens if 'http' in t.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60971134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized documents:\n",
      "Doc 1: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Doc 2: ['the', 'dog', 'sat', 'on', 'the', 'log']\n",
      "Doc 3: ['cats', 'and', 'dogs', 'are', 'enemies']\n",
      "\n",
      "Vocabulary (|V| = 12):\n",
      "['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'enemies', 'log', 'mat', 'on', 'sat', 'the']\n",
      "\n",
      "Bag-of-Words Matrix:\n",
      "       and  are  cat  cats  dog  dogs  enemies  log  mat  on  sat  the\n",
      "Doc 1    0    0    1     0    0     0        0    0    1   1    1    2\n",
      "Doc 2    0    0    0     0    1     0        0    1    0   1    1    2\n",
      "Doc 3    1    1    0     1    0     1        1    0    0   0    0    0\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 2: Bag-of-Words (BoW)\n",
    "#2.1 Manual BoW Construction\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    " \"The cat sat on the mat\",\n",
    " \"The dog sat on the log\",\n",
    " \"Cats and dogs are enemies\"\n",
    "]\n",
    "\n",
    "# Tokenize all documents\n",
    "all_tokens = []\n",
    "for doc in documents:\n",
    " tokens = word_tokenize(doc.lower())\n",
    " all_tokens.append(tokens)\n",
    "print(\"Tokenized documents:\")\n",
    "for i, tokens in enumerate(all_tokens):\n",
    " print(f\"Doc {i+1}: {tokens}\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = sorted(set([token for doc in all_tokens for token in doc]))\n",
    "print(f\"\\nVocabulary (|V| = {len(vocabulary)}):\")\n",
    "print(vocabulary)\n",
    "\n",
    "# Create BoW matrix\n",
    "bow_matrix = []\n",
    "for tokens in all_tokens:\n",
    " counts = Counter(tokens)\n",
    " bow_vector = [counts.get(word, 0) for word in vocabulary]\n",
    " bow_matrix.append(bow_vector)\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(bow_matrix, columns=vocabulary, index=[f\"Doc {i+1}\" for\n",
    "i in range(len(documents))])\n",
    "print(\"\\nBag-of-Words Matrix:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
