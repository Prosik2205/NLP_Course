{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78b71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK version: 3.9.2\n",
      "spaCy version: 3.8.11\n",
      "scikit-learn version: 1.8.0\n",
      "spaCy model loaded: core_web_sm\n"
     ]
    }
   ],
   "source": [
    "#SETUP & INSTALLATION\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"NLTK version: {nltk.__version__}\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Test spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello, world!\")\n",
    "print(f\"spaCy model loaded: {nlp.meta['name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc7589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 7\n",
      "\n",
      "Sentence 1: Natural Language Processing is fascinating.\n",
      "Sentence 2: It enables computers\n",
      "to understand human language.\n",
      "Sentence 3: Dr. Smith works at N.A.S.A.\n",
      "Sentence 4: on text analysis.\n",
      "Sentence 5: He said, \"NLP is the\n",
      "future!\"\n",
      "Sentence 6: What do you think?\n",
      "Sentence 7: Visit www.nlp.org for more info.\n"
     ]
    }
   ],
   "source": [
    "#Ex1\n",
    "#1.1\n",
    "\n",
    "import nltk\n",
    "# Download required NLTK data (run once)\n",
    "nltk.data.path.append(r\"C:\\Users\\prosi\\AppData\\Roaming\\nltk_data\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"Natural Language Processing is fascinating. It enables computers\n",
    "to understand human language.\n",
    "Dr. Smith works at N.A.S.A. on text analysis. He said, \"NLP is the\n",
    "future!\"\n",
    "What do you think? Visit www.nlp.org for more info.\"\"\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Number of sentences: {len(sentences)}\\n\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    " print(f\"Sentence {i}: {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 15\n",
      "\n",
      "Tokens: ['Do', \"n't\", 'forget', ':', 'pre-processing', 'costs', '$', '100-', '$', '200', '!', 'Email', 'john', '@', 'example.com']\n"
     ]
    }
   ],
   "source": [
    "#1.2\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"Don't forget: pre-processing costs $100-$200! Email john@example.com\"\n",
    "\n",
    "# Tokenize into words\n",
    "tokens = word_tokenize(sentence)\n",
    "print(f\"Number of tokens: {len(tokens)}\\n\")\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848cf7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard word_tokenize:\n",
      "['We', \"'re\", 'analyzing', 'BERT', \"'s\", 'performance', 'on', 'GPT-3.5', '.', 'Wow', '!']\n",
      "\n",
      "WordPunct tokenizer:\n",
      "['We', \"'\", 're', 'analyzing', 'BERT', \"'\", 's', 'performance', 'on', 'GPT', '-', '3', '.', '5', '.', 'Wow', '!']\n",
      "\n",
      "Treebank tokenizer:\n",
      "['We', \"'re\", 'analyzing', 'BERT', \"'s\", 'performance', 'on', 'GPT-3.5.', 'Wow', '!']\n"
     ]
    }
   ],
   "source": [
    "#1.3\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize,TreebankWordTokenizer\n",
    "text = \"We're analyzing BERT's performance on GPT-3.5. Wow!\"\n",
    "\n",
    "# Different tokenizers\n",
    "standard = word_tokenize(text)\n",
    "wordpunct = wordpunct_tokenize(text)\n",
    "treebank = TreebankWordTokenizer().tokenize(text)\n",
    "print(\"Standard word_tokenize:\")\n",
    "print(standard)\n",
    "print(f\"\\nWordPunct tokenizer:\")\n",
    "print(wordpunct)\n",
    "print(f\"\\nTreebank tokenizer:\")\n",
    "print(treebank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab5e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with POS tags and lemmas:\n",
      "\n",
      "Apple           | POS: PROPN    | Lemma:Apple           | Is_alpha: True\n",
      "Inc.            | POS: PROPN    | Lemma:Inc.            | Is_alpha: False\n",
      "is              | POS: AUX      | Lemma:be              | Is_alpha: True\n",
      "looking         | POS: VERB     | Lemma:look            | Is_alpha: True\n",
      "at              | POS: ADP      | Lemma:at              | Is_alpha: True\n",
      "buying          | POS: VERB     | Lemma:buy             | Is_alpha: True\n",
      "U.K.            | POS: PROPN    | Lemma:U.K.            | Is_alpha: False\n",
      "startup         | POS: VERB     | Lemma:startup         | Is_alpha: True\n",
      "for             | POS: ADP      | Lemma:for             | Is_alpha: True\n",
      "$               | POS: SYM      | Lemma:$               | Is_alpha: False\n",
      "1               | POS: NUM      | Lemma:1               | Is_alpha: False\n",
      "billion         | POS: NUM      | Lemma:billion         | Is_alpha: True\n",
      ".               | POS: PUNCT    | Lemma:.               | Is_alpha: False\n",
      "CEO             | POS: NOUN     | Lemma:ceo             | Is_alpha: True\n",
      "Tim             | POS: PROPN    | Lemma:Tim             | Is_alpha: True\n",
      "Cook            | POS: PROPN    | Lemma:Cook            | Is_alpha: True\n",
      "confirmed       | POS: VERB     | Lemma:confirm         | Is_alpha: True\n",
      "it              | POS: PRON     | Lemma:it              | Is_alpha: True\n",
      ".               | POS: PUNCT    | Lemma:.               | Is_alpha: False\n"
     ]
    }
   ],
   "source": [
    "#1.4\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple Inc. is looking at buying U.K. startup for $1 billion. CEO Tim Cook confirmed it.\"\n",
    "doc = nlp(text)\n",
    "print(\"Tokens with POS tags and lemmas:\\n\")\n",
    "for token in doc:\n",
    " print(f\"{token.text:15} | POS: {token.pos_:8} | Lemma:{token.lemma_:15} | Is_alpha: {token.is_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 4\n",
      "Total tokens: 33\n",
      "Unique tokens (types): 29\n",
      "\n",
      "URL handling:\n",
      "['https']\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 1: Your Turn ?????????\n",
    "text = \"\"\"\n",
    "The COVID-19 pandemic started in 2019-2020. Dr. Johnson said, \"We're\n",
    "making progress!\"\n",
    "The vaccine costs €50-€100 in the E.U. Visit https://who.int for updates.\n",
    "\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# 1. Sentence count\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentences: {len(sentences)}\")\n",
    "\n",
    "# 2. Word tokens\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "# 3. Unique tokens\n",
    "unique_tokens = set(tokens)\n",
    "print(f\"Unique tokens (types): {len(unique_tokens)}\")\n",
    "\n",
    "# 4. Check URL\n",
    "print(f\"\\nURL handling:\")\n",
    "print([t for t in tokens if 'http' in t.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60971134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized documents:\n",
      "Doc 1: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Doc 2: ['the', 'dog', 'sat', 'on', 'the', 'log']\n",
      "Doc 3: ['cats', 'and', 'dogs', 'are', 'enemies']\n",
      "\n",
      "Vocabulary (|V| = 12):\n",
      "['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'enemies', 'log', 'mat', 'on', 'sat', 'the']\n",
      "\n",
      "Bag-of-Words Matrix:\n",
      "       and  are  cat  cats  dog  dogs  enemies  log  mat  on  sat  the\n",
      "Doc 1    0    0    1     0    0     0        0    0    1   1    1    2\n",
      "Doc 2    0    0    0     0    1     0        0    1    0   1    1    2\n",
      "Doc 3    1    1    0     1    0     1        1    0    0   0    0    0\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 2: Bag-of-Words (BoW)\n",
    "#2.1 Manual BoW Construction\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    " \"The cat sat on the mat\",\n",
    " \"The dog sat on the log\",\n",
    " \"Cats and dogs are enemies\"\n",
    "]\n",
    "\n",
    "# Tokenize all documents\n",
    "all_tokens = []\n",
    "for doc in documents:\n",
    " tokens = word_tokenize(doc.lower())\n",
    " all_tokens.append(tokens)\n",
    "print(\"Tokenized documents:\")\n",
    "for i, tokens in enumerate(all_tokens):\n",
    " print(f\"Doc {i+1}: {tokens}\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = sorted(set([token for doc in all_tokens for token in doc]))\n",
    "print(f\"\\nVocabulary (|V| = {len(vocabulary)}):\")\n",
    "print(vocabulary)\n",
    "\n",
    "# Create BoW matrix\n",
    "bow_matrix = []\n",
    "for tokens in all_tokens:\n",
    " counts = Counter(tokens)\n",
    " bow_vector = [counts.get(word, 0) for word in vocabulary]\n",
    " bow_matrix.append(bow_vector)\n",
    "\n",
    "# Display as DataFrame\n",
    "df = pd.DataFrame(bow_matrix, columns=vocabulary, index=[f\"Doc {i+1}\" for\n",
    "i in range(len(documents))])\n",
    "print(\"\\nBag-of-Words Matrix:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f188cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix (CountVectorizer):\n",
      "      and  are  cat  cats  dog  dogs  enemies  log  mat  on  sat  the\n",
      "Doc1    0    0    1     0    0     0        0    0    1   1    1    2\n",
      "Doc2    0    0    0     0    1     0        0    1    0   1    1    2\n",
      "Doc3    1    1    0     1    0     1        1    0    0   0    0    0\n",
      "\n",
      "Vocabulary size: 12\n",
      "Matrix shape: (3, 12)\n",
      "Matrix sparsity: 58.3%\n"
     ]
    }
   ],
   "source": [
    "#2.2 scikit-learn CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\n",
    " \"The cat sat on the mat\",\n",
    " \"The dog sat on the log\",\n",
    " \"Cats and dogs are enemies\"\n",
    "]\n",
    "\n",
    "# Create CountVectorizer\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "\n",
    "# Fit and transform\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(bow_matrix.toarray(), columns=vocabulary, index=[f\"Doc{i+1}\" for i in range(len(documents))])\n",
    "print(\"Bag-of-Words Matrix (CountVectorizer):\")\n",
    "print(df)\n",
    "print(f\"\\nVocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Matrix shape: {bow_matrix.shape}\")\n",
    "print(f\"Matrix sparsity: {100 * (1 - bow_matrix.nnz / (bow_matrix.shape[0]\n",
    "* bow_matrix.shape[1])):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2a30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram vocabulary:\n",
      "['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'enemies' 'log' 'mat' 'on' 'sat'\n",
      " 'the']\n",
      "Size: 12\n",
      "\n",
      "Unigram + Bigram vocabulary:\n",
      "['and' 'and dogs' 'are' 'are enemies' 'cat' 'cat sat' 'cats' 'cats and'\n",
      " 'dog' 'dog sat' 'dogs' 'dogs are' 'enemies' 'log' 'mat' 'on' 'on the'\n",
      " 'sat' 'sat on' 'the' 'the cat' 'the dog' 'the log' 'the mat']\n",
      "Size: 24\n",
      "\n",
      "Bigram BoW Matrix:\n",
      "       and  and dogs  are  are enemies  cat  cat sat  cats  cats and  dog  \\\n",
      "Doc 1    0         0    0            0    1        1     0         0    0   \n",
      "Doc 2    0         0    0            0    0        0     0         0    1   \n",
      "Doc 3    1         1    1            1    0        0     1         1    0   \n",
      "\n",
      "       dog sat  ...  mat  on  on the  sat  sat on  the  the cat  the dog  \\\n",
      "Doc 1        0  ...    1   1       1    1       1    2        1        0   \n",
      "Doc 2        1  ...    0   1       1    1       1    2        0        1   \n",
      "Doc 3        0  ...    0   0       0    0       0    0        0        0   \n",
      "\n",
      "       the log  the mat  \n",
      "Doc 1        0        1  \n",
      "Doc 2        1        0  \n",
      "Doc 3        0        0  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# BoW with N-grams\n",
    "# Unigrams only\n",
    "vectorizer_unigram = CountVectorizer(ngram_range=(1, 1))\n",
    "bow_unigram = vectorizer_unigram.fit_transform(documents)\n",
    "\n",
    "# Unigrams + Bigrams\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(1, 2))\n",
    "bow_bigram = vectorizer_bigram.fit_transform(documents)\n",
    "print(\"Unigram vocabulary:\")\n",
    "print(vectorizer_unigram.get_feature_names_out())\n",
    "print(f\"Size: {len(vectorizer_unigram.get_feature_names_out())}\")\n",
    "print(\"\\nUnigram + Bigram vocabulary:\")\n",
    "print(vectorizer_bigram.get_feature_names_out())\n",
    "print(f\"Size: {len(vectorizer_bigram.get_feature_names_out())}\")\n",
    "\n",
    "# Display bigram matrix\n",
    "df_bigram = pd.DataFrame(\n",
    " bow_bigram.toarray(),\n",
    " columns=vectorizer_bigram.get_feature_names_out(),\n",
    " index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "print(\"\\nBigram BoW Matrix:\")\n",
    "print(df_bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef6cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common word: 'amazing' (count:2)\n",
      "Vocabulary size: 18\n",
      "Sparsity: 70.8%\n",
      "\n",
      "BoW Matrix:\n",
      "          acting  amazing  and  boring  but  cinematography  film  great  is  \\\n",
      "Review 1       0        1    1       0    0               0     0      0   1   \n",
      "Review 2       0        0    0       0    0               0     1      0   0   \n",
      "Review 3       1        0    0       1    1               0     0      1   0   \n",
      "Review 4       0        1    1       0    0               1     0      1   0   \n",
      "\n",
      "          movie  of  plot  story  terrible  this  time  waste  wonderful  \n",
      "Review 1      1   0     0      0         0     1     0      0          1  \n",
      "Review 2      0   1     0      0         1     0     1      1          0  \n",
      "Review 3      0   0     1      0         0     0     0      0          0  \n",
      "Review 4      0   0     0      1         0     0     0      0          0  \n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 2: Your Turn\n",
    "\n",
    "reviews = [\n",
    " \"This movie is amazing and wonderful\",\n",
    " \"Terrible film, waste of time\",\n",
    " \"Great acting but boring plot\",\n",
    " \"Amazing cinematography and great story\"\n",
    "]\n",
    "# TODO: Your code here\n",
    "# 1. Create BoW matrix with CountVectorizer\n",
    "# 2. Find the most common word across all reviews\n",
    "# 3. Calculate vocabulary size\n",
    "# 4. Compute sparsity\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Create BoW\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# 2. Most common word\n",
    "word_counts = bow.toarray().sum(axis=0)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "most_common_idx = word_counts.argmax()\n",
    "print(f\"Most common word: '{vocab[most_common_idx]}' (count:{word_counts[most_common_idx]})\")\n",
    "\n",
    "# 3. Vocabulary size\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# 4. Sparsity\n",
    "sparsity = 100 * (1 - bow.nnz / (bow.shape[0] * bow.shape[1]))\n",
    "print(f\"Sparsity: {sparsity:.1f}%\")\n",
    "\n",
    "# Display matrix\n",
    "df = pd.DataFrame(bow.toarray(), columns=vocab, index=[f\"Review {i+1}\" for\n",
    "i in range(len(reviews))])\n",
    "print(\"\\nBoW Matrix:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b72194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'friends', 'log', 'mat', 'on', 'sat', 'the']\n",
      "\n",
      "TF for Document 1:\n",
      " the       : 0.333\n",
      " cat       : 0.167\n",
      " mat       : 0.167\n",
      " on        : 0.167\n",
      " sat       : 0.167\n",
      "\n",
      "IDF scores:\n",
      " the       : 0.405\n",
      "\n",
      "TF-IDF for Document 1:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'and'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Calculate TF-IDF for Document 1\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTF-IDF for Document 1:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m tfidf_doc1 = {word: tf_doc1[word] * \u001b[43midf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocab}\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tfidf_doc1.items(), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m],\n\u001b[32m     45\u001b[39m reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[32m5\u001b[39m]:\n\u001b[32m     46\u001b[39m  \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m10s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'and'"
     ]
    }
   ],
   "source": [
    "#EXERCISE 3: TF-IDF\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "documents = [\n",
    " \"the cat sat on the mat\",\n",
    " \"the dog sat on the log\",\n",
    " \"cats and dogs are friends\"\n",
    "]\n",
    "# Tokenize\n",
    "tokenized = [doc.lower().split() for doc in documents]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = sorted(set([word for doc in tokenized for word in doc]))\n",
    "print(f\"Vocabulary: {vocab}\\n\")\n",
    "\n",
    "# Calculate TF (Term Frequency)\n",
    "def calculate_tf(doc_tokens, vocab):\n",
    " tf = {}\n",
    " doc_length = len(doc_tokens)\n",
    " for word in vocab:\n",
    "  tf[word] = doc_tokens.count(word) / doc_length\n",
    " return tf\n",
    "\n",
    "# Calculate IDF (Inverse Document Frequency)\n",
    "def calculate_idf(tokenized_docs, vocab):\n",
    " idf = {}\n",
    " N = len(tokenized_docs)\n",
    " for word in vocab:\n",
    "  doc_count = sum(1 for doc in tokenized_docs if word in doc)\n",
    " idf[word] = math.log(N / doc_count)\n",
    " return idf\n",
    "\n",
    "# Calculate TF for first document\n",
    "tf_doc1 = calculate_tf(tokenized[0], vocab)\n",
    "print(\"TF for Document 1:\")\n",
    "for word, score in sorted(tf_doc1.items(), key=lambda x: x[1],\n",
    "reverse=True)[:5]:\n",
    " print(f\" {word:10s}: {score:.3f}\")\n",
    " \n",
    "# Calculate IDF\n",
    "idf = calculate_idf(tokenized, vocab)\n",
    "print(\"\\nIDF scores:\")\n",
    "for word, score in sorted(idf.items(), key=lambda x: x[1], reverse=True) [:5]:print(f\" {word:10s}: {score:.3f}\")\n",
    " \n",
    "# Calculate TF-IDF for Document 1\n",
    "print(\"\\nTF-IDF for Document 1:\")\n",
    "tfidf_doc1 = {word: tf_doc1[word] * idf[word] for word in vocab} ##tf_doc1.get(word, 0) * idf.get(word, 0)\n",
    "for word, score in sorted(tfidf_doc1.items(), key=lambda x: x[1],\n",
    "reverse=True)[:5]:\n",
    " print(f\" {word:10s}: {score:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16d4534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "        and    are    cat   cats    dog   dogs  friends    log    mat     on  \\\n",
      "Doc1  0.000  0.000  1.693  0.000  0.000  0.000    0.000  0.000  1.693  1.288   \n",
      "Doc2  0.000  0.000  0.000  0.000  1.693  0.000    0.000  1.693  0.000  1.288   \n",
      "Doc3  1.693  1.693  0.000  1.693  0.000  1.693    1.693  0.000  0.000  0.000   \n",
      "\n",
      "        sat    the  \n",
      "Doc1  1.288  2.575  \n",
      "Doc2  1.288  2.575  \n",
      "Doc3  0.000  0.000  \n",
      "\n",
      "Top 3 words per document:\n",
      "Doc 1: the cat sat on the mat\n",
      " the       : 2.575\n",
      " mat       : 1.693\n",
      " cat       : 1.693\n",
      "Doc 2: the dog sat on the log\n",
      " the       : 2.575\n",
      " log       : 1.693\n",
      " dog       : 1.693\n",
      "Doc 3: cats and dogs are friends\n",
      " cats      : 1.693\n",
      " are       : 1.693\n",
      " dogs      : 1.693\n"
     ]
    }
   ],
   "source": [
    "#3.2 scikit-learn TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "documents = [\n",
    " \"the cat sat on the mat\",\n",
    " \"the dog sat on the log\",\n",
    " \"cats and dogs are friends\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, norm=None) # norm=None for raw TF-IDF\n",
    "    \n",
    "# Fit and transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display as DataFrame\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vocab, index=[f\"Doc{i+1}\" for i in range(len(documents))])\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(df.round(3))\n",
    "\n",
    "# Show highest TF-IDF words per document\n",
    "print(\"\\nTop 3 words per document:\")\n",
    "for i, doc in enumerate(documents):\n",
    " tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
    " top_indices = tfidf_scores.argsort()[-3:][::-1]\n",
    " top_words = [(vocab[idx], tfidf_scores[idx]) for idx in top_indices]\n",
    " print(f\"Doc {i+1}: {doc}\")\n",
    " for word, score in top_words:\n",
    "  print(f\" {word:10s}: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7bde63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for word 'programming':\n",
      " BoW: [1 1 0 0]\n",
      " TF-IDF: [0.325 0.413 0.    0.   ]\n",
      "\n",
      "Scores for word 'is':\n",
      " BoW: [1 1 0 1]\n",
      " TF-IDF: [0.263 0.334 0.    0.278]\n"
     ]
    }
   ],
   "source": [
    "#3.3 Comparing BoW vs TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "documents = [\n",
    " \"Python is a great programming language for data science\",\n",
    " \"Java is also a programming language\",\n",
    " \"Data science requires knowledge of statistics and machine learning\",\n",
    " \"Machine learning is a subset of artificial intelligence\"\n",
    "]\n",
    "\n",
    "# BoW\n",
    "bow_vec = CountVectorizer()\n",
    "bow_matrix = bow_vec.fit_transform(documents)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n",
    "\n",
    "# Compare scores for \"programming\"\n",
    "vocab_bow = bow_vec.get_feature_names_out()\n",
    "vocab_tfidf = tfidf_vec.get_feature_names_out()\n",
    "word = \"programming\"\n",
    "bow_idx = list(vocab_bow).index(word)\n",
    "tfidf_idx = list(vocab_tfidf).index(word)\n",
    "print(f\"Scores for word '{word}':\")\n",
    "print(f\" BoW: {bow_matrix[:, bow_idx].toarray().flatten()}\")\n",
    "print(f\" TF-IDF: {tfidf_matrix[:,\n",
    "tfidf_idx].toarray().flatten().round(3)}\")\n",
    "\n",
    "# Compare common words\n",
    "word_common = \"is\"\n",
    "bow_idx = list(vocab_bow).index(word_common)\n",
    "tfidf_idx = list(vocab_tfidf).index(word_common)\n",
    "print(f\"\\nScores for word '{word_common}':\")\n",
    "print(f\" BoW: {bow_matrix[:, bow_idx].toarray().flatten()}\")\n",
    "print(f\" TF-IDF: {tfidf_matrix[:,\n",
    "tfidf_idx].toarray().flatten().round(3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8c6fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 distinctive words per document:\n",
      "Doc 1: Machine learning algorithms require large datasets\n",
      " algorithms     : 0.462\n",
      " require        : 0.462\n",
      "Doc 2: Deep learning is a subset of machine learning\n",
      " learning       : 0.461\n",
      " subset         : 0.442\n",
      "Doc 3: Natural language processing uses machine learning\n",
      " uses           : 0.462\n",
      " processing     : 0.462\n",
      "Doc 4: Computer vision applies deep learning techniques\n",
      " vision         : 0.452\n",
      " techniques     : 0.452\n",
      "\n",
      "Highest IDF word: 'algorithms' (IDF:1.916)\n",
      "This word appears in the fewest documents\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 3: Your Turn\n",
    "documents = [\n",
    " \"Machine learning algorithms require large datasets\",\n",
    " \"Deep learning is a subset of machine learning\",\n",
    " \"Natural language processing uses machine learning\",\n",
    " \"Computer vision applies deep learning techniques\"\n",
    "]\n",
    "# TODO: Your code here\n",
    "# 1. Calculate TF-IDF\n",
    "# 2. Find top 2 most distinctive words per document\n",
    "# 3. Which word has highest IDF? (appears in fewest documents)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Calculate TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(documents)\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "# 2. Top 2 words per document\n",
    "print(\"Top 2 distinctive words per document:\")\n",
    "for i in range(len(documents)):\n",
    " scores = tfidf_matrix[i].toarray().flatten()\n",
    " top_indices = scores.argsort()[-2:][::-1]\n",
    " print(f\"Doc {i+1}: {documents[i]}\")\n",
    " for idx in top_indices:\n",
    "  print(f\" {vocab[idx]:15s}: {scores[idx]:.3f}\")\n",
    "\n",
    "# 3. Highest IDF word\n",
    "idf_scores = tfidf.idf_\n",
    "max_idf_idx = idf_scores.argmax()\n",
    "print(f\"\\nHighest IDF word: '{vocab[max_idf_idx]}' (IDF:{idf_scores[max_idf_idx]:.3f})\")\n",
    "print(\"This word appears in the fewest documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c000a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased: hello world! this is nlp 101. are you ready? let's go!!!\n",
      "No punctuation: hello world this is nlp 101 are you ready lets go\n",
      "Tokens (alphabetic only): ['hello', 'world', 'this', 'is', 'nlp', 'are', 'you', 'ready', 'let', 'go']\n"
     ]
    }
   ],
   "source": [
    "#4.1 Lowercasing and Punctuation Removal\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello World! This is NLP 101. Are you ready? Let's GO!!!\"\n",
    "\n",
    "# Lowercase\n",
    "text_lower = text.lower()\n",
    "print(f\"Lowercased: {text_lower}\")\n",
    "\n",
    "# Remove punctuation (method 1: translate)\n",
    "text_no_punct = text_lower.translate(str.maketrans('', '',\n",
    "string.punctuation))\n",
    "print(f\"No punctuation: {text_no_punct}\")\n",
    "\n",
    "# Remove punctuation (method 2: during tokenization)\n",
    "tokens = word_tokenize(text_lower)\n",
    "tokens_alpha = [t for t in tokens if t.isalpha()]\n",
    "print(f\"Tokens (alphabetic only): {tokens_alpha}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9946efbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['this', 'is', 'a', 'sample', 'sentence', 'demonstrating', 'the', 'removal', 'of', 'stop', 'words']\n",
      "\n",
      "Number of English stop words: 198\n",
      "Sample stop words: ['hasn', 'now', 'are', 'its', \"she'll\", \"wouldn't\", 'ours', \"they'll\", 'so', 'up']\n",
      "\n",
      "Filtered tokens: ['sample', 'sentence', 'demonstrating', 'removal', 'stop', 'words']\n",
      "Removed 5 stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prosi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#4.2 Stop Words Removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "text = \"This is a sample sentence demonstrating the removal of stop words\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(f\"Original tokens: {tokens}\")\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"\\nNumber of English stop words: {len(stop_words)}\")\n",
    "print(f\"Sample stop words: {list(stop_words)[:10]}\")\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "print(f\"\\nFiltered tokens: {filtered_tokens}\")\n",
    "print(f\"Removed {len(tokens) - len(filtered_tokens)} stop words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9367d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word | Stem\n",
      "------------------------------\n",
      "running        | run\n",
      "runs           | run\n",
      "ran            | ran\n",
      "runner         | runner\n",
      "easily         | easili\n",
      "fairly         | fairli\n",
      "happiness      | happi\n",
      "happily        | happili\n",
      "happier        | happier\n"
     ]
    }
   ],
   "source": [
    "#4.3 Stemming (Porter Stemmer)\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\",\n",
    "\"happiness\", \"happily\", \"happier\"]\n",
    "print(\"Word | Stem\")\n",
    "print(\"-\" * 30)\n",
    "for word in words:\n",
    " stem = stemmer.stem(word)\n",
    " print(f\"{word:15s}| {stem}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfed6926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prosi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\prosi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word | Lemma (no POS) | Lemma (with POS)\n",
      "------------------------------------------------------------\n",
      "running        | running        | run\n",
      "runs           | run            | run\n",
      "ran            | ran            | run\n",
      "runner         | runner         | runner\n",
      "better         | better         | better\n",
      "best           | best           | best\n",
      "good           | good           | good\n",
      "am             | am             | be\n",
      "is             | is             | be\n",
      "are            | are            | be\n"
     ]
    }
   ],
   "source": [
    "#4.4 Lemmatization (WordNet)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"runs\", \"ran\", \"runner\", \"better\", \"best\", \"good\",\n",
    "\"am\", \"is\", \"are\"]\n",
    "print(\"Word | Lemma (no POS) | Lemma (with POS)\")\n",
    "print(\"-\" * 60)\n",
    "for word in words:\n",
    " lemma_no_pos = lemmatizer.lemmatize(word)\n",
    " lemma_verb = lemmatizer.lemmatize(word, pos='v') # verb\n",
    " print(f\"{word:15s}| {lemma_no_pos:15s}| {lemma_verb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c62c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The children are playing in the park. They're having so much fun!\n",
      "\n",
      "Preprocessed tokens:\n",
      "['child', 'playing', 'park', 'much', 'fun']\n",
      "\n",
      "Reconstructed text:\n",
      "child playing park much fun\n"
     ]
    }
   ],
   "source": [
    "#4.5 Complete Preprocessing Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    " \"\"\"\n",
    " Complete text preprocessing pipeline\n",
    " \"\"\"\n",
    " \n",
    " # 1. Lowercase\n",
    " text = text.lower()\n",
    "\n",
    " # 2. Tokenize\n",
    " tokens = word_tokenize(text)\n",
    "\n",
    " # 3. Remove punctuation and non-alphabetic tokens\n",
    " tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    " # 4. Remove stop words\n",
    " if remove_stopwords:\n",
    "  stop_words = set(stopwords.words('english'))\n",
    " tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    " # 5. Lemmatize\n",
    " if lemmatize:\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    " tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    " return tokens\n",
    "\n",
    "# Test the pipeline\n",
    "text = \"The children are playing in the park. They're having so much fun!\"\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\nPreprocessed tokens:\")\n",
    "tokens = preprocess_text(text)\n",
    "print(tokens)\n",
    "print(\"\\nReconstructed text:\")\n",
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50ef238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "I absolutely LOVED this movie!!! The acting was great, but the plot was terrible. Would not recommend. 2/10\n",
      "\n",
      "Processed tokens:\n",
      "['i', 'absolutely', 'loved', 'this', 'movie', 'the', 'acting', 'wa', 'great', 'but', 'the', 'plot', 'wa', 'terrible', 'would', 'not', 'recommend']\n",
      "\n",
      "Token count: 17\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE 4: Your Turn\n",
    "review = \"I absolutely LOVED this movie!!! The acting was great, but the plot was terrible. Would not recommend. 2/10\"\n",
    "# TODO: Your code here\n",
    "# Requirements:\n",
    "# 1. Lowercase\n",
    "# 2. Remove punctuation and numbers\n",
    "# 3. Tokenize\n",
    "# 4. Keep stop words (important for sentiment!)\n",
    "# 5. Lemmatize\n",
    "# 6. Print before/after\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess_sentiment(text):\n",
    " # 1. Lowercase\n",
    " text = text.lower()\n",
    "\n",
    " # 2. Tokenize\n",
    " tokens = word_tokenize(text)\n",
    "\n",
    " # 3. Remove punctuation and numbers, keep alphabetic\n",
    " tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    " # 4. NO stop word removal (negations matter for sentiment!)\n",
    "\n",
    " # 5. Lemmatize\n",
    " lemmatizer = WordNetLemmatizer()\n",
    " tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    " return tokens\n",
    "print(\"Original:\")\n",
    "print(review)\n",
    "tokens = preprocess_sentiment(review)\n",
    "print(\"\\nProcessed tokens:\")\n",
    "print(tokens)\n",
    "print(f\"\\nToken count: {len(tokens)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d9c7444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token | Stem | Lemma\n",
      "--------------------------------------------------\n",
      "the            | the            | the\n",
      "striped        | stripe         | striped\n",
      "bats           | bat            | bat\n",
      "are            | are            | are\n",
      "hanging        | hang           | hanging\n",
      "on             | on             | on\n",
      "their          | their          | their\n",
      "feet           | feet           | foot\n",
      "for            | for            | for\n",
      "best           | best           | best\n",
      "performance    | perform        | performance\n",
      "the            | the            | the\n",
      "studies        | studi          | study\n",
      "have           | have           | have\n",
      "shown          | shown          | shown\n",
      "\n",
      "Processing time:\n",
      "Stemming: 1.00ms\n",
      "Lemmatization: 0.00ms\n",
      "Speed ratio: 0.0x slower\n",
      "\n",
      "Vocabulary size:\n",
      "Original: 27\n",
      "After stemming: 27\n",
      "After lemmatization: 27\n"
     ]
    }
   ],
   "source": [
    "#EXERCISE 5: Stemming vs Lemmatization Comparison\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "text = \"\"\"\n",
    "The striped bats are hanging on their feet for best performance.\n",
    "The studies have shown that running improves cardiovascular health.\n",
    "The children were playing with better toys than before.\n",
    "\"\"\"\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "start = time.time()\n",
    "stems = [stemmer.stem(t) for t in tokens]\n",
    "stem_time = time.time() - start\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "start = time.time()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "lemma_time = time.time() - start\n",
    "\n",
    "# Compare results\n",
    "print(\"Token | Stem | Lemma\")\n",
    "print(\"-\" * 50)\n",
    "for token, stem, lemma in zip(tokens[:15], stems[:15], lemmas[:15]):\n",
    " print(f\"{token:15s}| {stem:15s}| {lemma}\")\n",
    "print(f\"\\nProcessing time:\")\n",
    "print(f\"Stemming: {stem_time*1000:.2f}ms\")\n",
    "print(f\"Lemmatization: {lemma_time*1000:.2f}ms\")\n",
    "print(f\"Speed ratio: {lemma_time/stem_time:.1f}x slower\")\n",
    "\n",
    "# Vocabulary size comparison\n",
    "print(f\"\\nVocabulary size:\")\n",
    "print(f\"Original: {len(set(tokens))}\")\n",
    "print(f\"After stemming: {len(set(stems))}\")\n",
    "print(f\"After lemmatization: {len(set(lemmas))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "281eb761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Matrix:\n",
      "(1.0 = identical, 0.0 = no common words)\n",
      "\n",
      "Doc 1: 1.00 0.07 0.00 0.09 0.52 \n",
      "Doc 2: 0.07 1.00 0.00 0.00 0.24 \n",
      "Doc 3: 0.00 0.00 1.00 0.07 0.00 \n",
      "Doc 4: 0.09 0.00 0.07 1.00 0.08 \n",
      "Doc 5: 0.52 0.24 0.00 0.08 1.00 \n",
      "\n",
      "Most similar document pairs:\n",
      "Doc 1 ↔ Doc 5: 0.516\n",
      " Doc 1: Machine learning is a subset of artificial intelli...\n",
      "Doc 2 ↔ Doc 5: 0.236\n",
      " Doc 2: Deep learning uses neural networks with many layer...\n"
     ]
    }
   ],
   "source": [
    "#6.1 Document Similarity with TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Deep learning uses neural networks with many layers\",\n",
    "    \"\"\"Natural language processing helps computers understand human\n",
    "    language\"\"\",\n",
    "    \"\"\"Computer vision is about teaching computers to see and interpret\n",
    "    images\"\"\",\n",
    "    \"Artificial intelligence includes machine learning and deep learning\"\n",
    "]\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(documents)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Display similarity matrix\n",
    "print(\"Document Similarity Matrix:\")\n",
    "print(\"(1.0 = identical, 0.0 = no common words)\\n\")\n",
    "for i in range(len(documents)):\n",
    " print(f\"Doc {i+1}:\", end=\" \")\n",
    " for j in range(len(documents)):\n",
    "  print(f\"{similarity_matrix[i][j]:.2f}\", end=\" \")\n",
    " print()\n",
    "\n",
    "# Find most similar document pairs\n",
    "print(\"\\nMost similar document pairs:\")\n",
    "for i in range(len(documents)):\n",
    " for j in range(i+1, len(documents)):sim = similarity_matrix[i][j]\n",
    " if sim > 0.1: # Threshold\n",
    "  print(f\"Doc {i+1} ↔ Doc {j+1}: {sim:.3f}\")\n",
    "  print(f\" Doc {i+1}: {documents[i][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25ec50bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Keywords:\n",
      "python              : 0.756\n",
      "data                : 0.378\n",
      "analysis            : 0.189\n",
      "automation          : 0.189\n",
      "backend             : 0.189\n",
      "beginners           : 0.189\n",
      "community           : 0.189\n",
      "companies           : 0.189\n",
      "high                : 0.189\n",
      "ideal               : 0.189\n"
     ]
    }
   ],
   "source": [
    "#6.2 Keyword Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "document = \"\"\"\n",
    "Python is a high-level programming language. It is widely used for web\n",
    "development,\n",
    "data science, machine learning, and automation. Python's simple syntax\n",
    "makes it\n",
    "ideal for beginners. The Python community is large and supportive. Many\n",
    "companies\n",
    "use Python for their backend services and data analysis pipelines.\n",
    "\"\"\"\n",
    "# Extract keywords using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=10, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform([document])\n",
    "\n",
    "# Get feature names and scores\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "# Sort by score\n",
    "keyword_scores = sorted(zip(feature_names, scores), key=lambda x: x[1],reverse=True)\n",
    "print(\"Top 10 Keywords:\")\n",
    "for keyword, score in keyword_scores:\n",
    " print(f\"{keyword:20s}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee8f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
